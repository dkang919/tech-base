[
    {
        "category": "1. Regression",
        "q": "What are the key assumptions of Linear Regression?",
        "a": "1. <strong>Linearity</strong> between X and Y.<br>2. <strong>Independence</strong> of errors.<br>3. <strong>Homoscedasticity</strong> (constant variance of errors).<br>4. <strong>Normality</strong> of error distribution.<br>5. No multicollinearity."
    },
    {
        "category": "1. Regression",
        "q": "What is R-squared (Coefficient of Determination)?",
        "a": "It represents the <strong>proportion of variance</strong> in the dependent variable that is explained by the independent variables. It ranges from 0 to 1."
    },
    {
        "category": "1. Regression",
        "q": "What is the difference between R-squared and Adjusted R-squared?",
        "a": "R-squared always increases as you add more features, even irrelevant ones. <strong>Adjusted R-squared</strong> penalizes adding useless features, providing a more accurate measure for multiple regression."
    },
    {
        "category": "1. Regression",
        "q": "Explain the difference between L1 (Lasso) and L2 (Ridge) regularization.",
        "a": "L1 adds the <strong>absolute value</strong> of magnitude of coefficient as penalty term, leading to feature selection (sparse solution). L2 adds the <strong>squared magnitude</strong>, shrinking coefficients but not to zero."
    },
    {
        "category": "1. Regression",
        "q": "When would you use Lasso (L1) over Ridge (L2)?",
        "a": "When you suspect that many features are irrelevant. Lasso can drive coefficients to <strong>zero</strong>, effectively performing <strong>feature selection</strong>."
    },
    {
        "category": "1. Regression",
        "q": "What is Multicollinearity and why is it a problem?",
        "a": "It occurs when independent variables are highly correlated. It makes it difficult to determine the individual effect of each feature and makes the coefficients <strong>unstable</strong> with high variance."
    },
    {
        "category": "1. Regression",
        "q": "How can you detect Multicollinearity?",
        "a": "By using the <strong>Variance Inflation Factor (VIF)</strong>. A VIF value greater than 5 or 10 indicates high multicollinearity."
    },
    {
        "category": "2. Logistic Regression",
        "q": "Why is accuracy not a good metric for imbalanced datasets?",
        "a": "In a highly imbalanced dataset (e.g., 99% negative), a model predicting only 'negative' will have 99% accuracy but zero predictive power. Metrics like <strong>Precision, Recall, and F1-Score</strong> are better."
    },
    {
        "category": "2. Logistic Regression",
        "q": "What is the ROC Curve and AUC?",
        "a": "ROC plots <strong>True Positive Rate (Recall)</strong> vs <strong>False Positive Rate</strong> at various thresholds. AUC (Area Under Curve) measures the ability of the classifier to distinguish between classes."
    },
    {
        "category": "2. Logistic Regression",
        "q": "What is the interpretability of Logistic Regression coefficients?",
        "a": "The coefficients represent the change in the <strong>log-odds</strong> of the outcome for a one-unit increase in the predictor variable."
    },
    {
        "category": "3. Decision Tree",
        "q": "What is Entropy in the context of Decision Trees?",
        "a": "Entropy measures the <strong>impurity</strong> or randomness in a dataset. A lower entropy indicates a purer node (more homogenous class distribution)."
    },
    {
        "category": "3. Decision Tree",
        "q": "What is Information Gain?",
        "a": "It is the <strong>reduction in entropy</strong> achieved by splitting a dataset on a specific attribute. The tree chooses the split with the highest Information Gain."
    },
    {
        "category": "3. Decision Tree",
        "q": "What is Gini Impurity?",
        "a": "Another measure of impurity used in CART algorithms. It measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node."
    },
    {
        "category": "3. Decision Tree",
        "q": "How do you prevent overfitting in Decision Trees?",
        "a": "By <strong>Pruning</strong> (pre-pruning or post-pruning), setting a maximum depth, setting a minimum number of samples per leaf, or using ensemble methods."
    },
    {
        "category": "4. Random Forest",
        "q": "What is Bootstrap Aggregating (Bagging)?",
        "a": "It involves creating multiple subsets of data from the training set with replacement (Bootstrap) and training a model on each subset. The final prediction is an average (Aggregating)."
    },
    {
        "category": "4. Random Forest",
        "q": "Why does Random Forest generally outperform a single Decision Tree?",
        "a": "It reduces <strong>variance</strong>. By averaging many trees that are decorrelated, the model becomes more robust to noise and less prone to overfitting."
    },
    {
        "category": "4. Random Forest",
        "q": "What is the role of 'mtry' (number of features tried at each split)?",
        "a": "It controls the diversity of the trees. A smaller mtry makes trees more diverse (less correlated) but potentially individually weaker. It is a key hyperparameter to tune."
    },
    {
        "category": "5. Boosting (XGBoost)",
        "q": "How does Gradient Boosting differ from AdaBoost?",
        "a": "AdaBoost updates weights of data points (misclassified points get higher weight). Gradient Boosting fits new models to the <strong>residual errors</strong> (gradients) of the previous models."
    },
    {
        "category": "5. Boosting (XGBoost)",
        "q": "What is the 'Learning Rate' in Boosting?",
        "a": "It scales the contribution of each new tree. A lower learning rate requires more trees but generally leads to better generalization."
    },
    {
        "category": "5. Boosting (XGBoost)",
        "q": "Why is XGBoost so popular in Kaggle competitions?",
        "a": "Speed and performance. It uses system optimization (parallelization, cache awareness) and algorithmic enhancements (regularization, handling missing values, tree pruning)."
    },
    {
        "category": "6. SVM",
        "q": "What is the main objective of a Support Vector Machine (SVM)?",
        "a": "To find the <strong>hyperplane</strong> that maximizes the <strong>margin</strong> between two classes."
    },
    {
        "category": "6. SVM",
        "q": "What are Support Vectors?",
        "a": "They are the data points <strong>closest</strong> to the hyperplane. They are the only points that influence the position and orientation of the hyperplane."
    },
    {
        "category": "6. SVM",
        "q": "What is the 'Kernel Trick'?",
        "a": "It allows SVM to solve non-linear problems by mapping data into a <strong>higher-dimensional space</strong> where it becomes linearly separable, without computing the coordinates explicitly."
    },
    {
        "category": "6. SVM",
        "q": "What does the 'C' parameter control in SVM?",
        "a": "It controls the trade-off between maximizing the margin and minimizing classification errors. High C tries to classify all training examples correctly (low bias, high variance). Low C allows more errors for a wider margin."
    },
    {
        "category": "6. SVM",
        "q": "What does the 'Gamma' parameter control in RBF Kernel?",
        "a": "It defines how far the influence of a single training example reaches. High Gamma means only close points are considered (complex boundary, overfitting risk). Low Gamma means far points are considered."
    },
    {
        "category": "7. PCA",
        "q": "What is the goal of Principal Component Analysis (PCA)?",
        "a": "To reduce dimensionality while preserving as much <strong>variance</strong> (information) as possible by transforming variables into a new set of orthogonal variables (Principal Components)."
    },
    {
        "category": "7. PCA",
        "q": "Why is data standardization (scaling) important before PCA?",
        "a": "Because PCA is sensitive to the scale of features. It maximizes variance, so variables with large scales will dominate the principal components if not standardized."
    },
    {
        "category": "7. PCA",
        "q": "What are Eigenvalues and Eigenvectors in PCA?",
        "a": "Eigenvectors represent the <strong>direction</strong> of the axes (Principal Components). Eigenvalues represent the <strong>magnitude of variance</strong> explained by that component."
    },
    {
        "category": "8. Neural Networks",
        "q": "What is an Activation Function and why is it needed?",
        "a": "It introduces <strong>non-linearity</strong> to the network. Without it, a neural network, no matter how deep, would be equivalent to a single linear regression model."
    },
    {
        "category": "8. Neural Networks",
        "q": "Explain the Softmax function.",
        "a": "It is used in the output layer for multi-class classification. It converts raw scores (logits) into probabilities that sum up to 1."
    },
    {
        "category": "8. Neural Networks",
        "q": "What is Backpropagation?",
        "a": "The algorithm to compute the <strong>gradient of the loss function</strong> with respect to the weights. It propagates the error backward from output to input using the Chain Rule."
    },
    {
        "category": "8. Neural Networks",
        "q": "What is the Vanishing Gradient problem?",
        "a": "In deep networks, gradients can become infinitely small as they are multiplied during backprop, causing early layers to stop learning. <strong>ReLU</strong> and <strong>Batch Normalization</strong> help fix this."
    },
    {
        "category": "8. Neural Networks",
        "q": "What is Dropout?",
        "a": "A regularization technique where neurons are randomly ignored during training to prevent co-adaptation and overfitting."
    },
    {
        "category": "8. Neural Networks",
        "q": "What is Batch Normalization?",
        "a": "A technique to normalize the inputs of each layer to have a mean of 0 and variance of 1. It speeds up training and makes the network more stable."
    },
    {
        "category": "8. Neural Networks",
        "q": "Difference between Stochastic Gradient Descent (SGD) and Mini-batch GD?",
        "a": "SGD updates weights after <strong>every single sample</strong> (noisy, fast). Mini-batch GD updates after a <strong>small batch</strong> of samples (balance between speed and stability)."
    },
    {
        "category": "9. Model Evaluation",
        "q": "What is the Bias-Variance Tradeoff?",
        "a": "<strong>Bias</strong> is error due to overly simplistic assumptions (underfitting). <strong>Variance</strong> is error due to sensitivity to noise (overfitting). You want to find the sweet spot minimizing total error."
    },
    {
        "category": "9. Model Evaluation",
        "q": "What is Cross-Validation (e.g., K-Fold)?",
        "a": "A technique to assess how the results of a statistical analysis will generalize to an independent data set. It splits data into K folds, training on K-1 and testing on 1, K times."
    },
    {
        "category": "9. Model Evaluation",
        "q": "What is a Confusion Matrix?",
        "a": "A table used to evaluate classification models. It shows True Positives, True Negatives, False Positives (Type I error), and False Negatives (Type II error)."
    },
    {
        "category": "9. Model Evaluation",
        "q": "Recall vs Precision: Which is more important for cancer detection?",
        "a": "<strong>Recall</strong> is more important. You want to catch all cancer cases (minimize False Negatives), even if it means some healthy people are flagged for further testing (False Positives)."
    },
    {
        "category": "10. Clustering (Etc)",
        "q": "How does K-Means Clustering work?",
        "a": "1. Initialize K centroids.<br>2. Assign points to nearest centroid.<br>3. Recalculate centroids.<br>4. Repeat until convergence."
    },
    {
        "category": "10. Clustering (Etc)",
        "q": "How do you choose 'K' in K-Means?",
        "a": "Using the <strong>Elbow Method</strong>. Plot the Sum of Squared Errors (SSE) vs K, and look for the 'elbow' point where the reduction in SSE diminishes."
    },
    {
        "category": "10. Clustering (Etc)",
        "q": "Difference between K-Means and K-Nearest Neighbors (KNN)?",
        "a": "K-Means is <strong>Unsupervised</strong> (Clustering). KNN is <strong>Supervised</strong> (Classification/Regression). They are completely different algorithms despite similar names."
    }
]